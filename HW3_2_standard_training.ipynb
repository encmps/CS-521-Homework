{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/root/miniconda3/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/root/miniconda3/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/root/miniconda3/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/root/miniconda3/lib/python3.10/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchtext import datasets\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 64\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# reference: https://zhuanlan.zhihu.com/p/562565880\n",
    "train_dataset, test_dataset = datasets.IMDB(root='./IMDB_data', split=('train', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokenizer('UIUC Siebel School.')\n",
    "\n",
    "GLOVE_DIM = 100\n",
    "GLOVE = GloVe(name='6B', dim=GLOVE_DIM)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    labels, texts = list(), list()\n",
    "    for label, text in batch:\n",
    "        # print('label', label)\n",
    "        label = label - 1\n",
    "        assert label in [0, 1]\n",
    "        \n",
    "        tokens = tokenizer(text)\n",
    "        emb = list()\n",
    "        for token in tokens:\n",
    "            if token in GLOVE.stoi:\n",
    "                emb.append(GLOVE[token])\n",
    "        \n",
    "        labels.append(label)\n",
    "        # for e in emb:\n",
    "        #     print(e.shape)\n",
    "        texts.append(torch.stack(emb))\n",
    "        \n",
    "    labels = torch.tensor(labels)\n",
    "    # print(texts)\n",
    "    # print(f'len(texts)={len(texts)}')\n",
    "    texts = pad_sequence(texts, batch_first=True)\n",
    "    return texts, labels\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: IBP Modified Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(100, 100)\n",
    "        self.fc1 = nn.Linear(100, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.output = nn.Linear(100, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def _prop_affine(self, l_x, u_x, W, b):\n",
    "        sum = (u_x + l_x) / 2\n",
    "        dif = (u_x - l_x) / 2\n",
    "        \n",
    "        pos = torch.matmul(sum, W.T) + b.unsqueeze(0)\n",
    "        neg = torch.matmul(dif, torch.abs(W).T)\n",
    "        \n",
    "        l_x = pos - neg\n",
    "        u_x = pos + neg\n",
    "        return l_x, u_x\n",
    "    \n",
    "    def _prop_average(self, l_x, u_x, dim=2):\n",
    "        lower = torch.mean(l_x, dim=dim)\n",
    "        upper = torch.mean(u_x, dim=dim)\n",
    "        return lower, upper\n",
    "    \n",
    "    def _prop_relu(self, l_x, u_x):\n",
    "        l_x = torch.max(l_x, torch.zeros_like(l_x))\n",
    "        u_x = torch.max(u_x, torch.zeros_like(u_x))\n",
    "        return l_x, u_x\n",
    "    \n",
    "    def _prop_softmax(self, l_x, u_x, dim=1):\n",
    "        upper_1 = torch.exp(u_x)\n",
    "        upper_2 = torch.sum(torch.exp(l_x), dim=dim, keepdim=True) - torch.exp(l_x) + torch.exp(u_x)\n",
    "        upper = upper_1 / upper_2\n",
    "        \n",
    "        lower_1 = torch.exp(l_x)\n",
    "        lower_2 = torch.sum(torch.exp(u_x), dim=dim, keepdim=True) - torch.exp(u_x) + torch.exp(l_x)\n",
    "        lower = lower_1 / lower_2\n",
    "        return lower, upper\n",
    "    \n",
    "    def forward(self, x, l_x, u_x):\n",
    "        # print(1, l_x.shape)\n",
    "        l_x, u_x = self._prop_affine(l_x, u_x, self.fc0.weight, self.fc0.bias)\n",
    "        # print(2, l_x.shape)\n",
    "        l_x, u_x = self._prop_relu(l_x, u_x)\n",
    "        l_x, u_x = self._prop_average(l_x, u_x, dim=1)\n",
    "        # print(3, l_x.shape)\n",
    "        \n",
    "        l_x, u_x = self._prop_affine(l_x, u_x, self.fc1.weight, self.fc1.bias)\n",
    "        l_x, u_x = self._prop_relu(l_x, u_x)\n",
    "        l_x, u_x = self._prop_affine(l_x, u_x, self.fc2.weight, self.fc2.bias)\n",
    "        l_x, u_x = self._prop_relu(l_x, u_x)\n",
    "        l_y, u_y = self._prop_affine(l_x, u_x, self.output.weight, self.output.bias)\n",
    "        l_y, u_y = self._prop_softmax(l_y, u_y, dim=1)\n",
    "        \n",
    "        x = self.fc0(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x1 = F.relu(self.fc1(x))\n",
    "        x2 = F.relu(self.fc2(x1))\n",
    "        y = self.softmax(self.output(x2))\n",
    "        \n",
    "        return y, l_y, u_y\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "def criterion(x, l_x, u_x, y, kappa):\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    l_fit = cel(x, y)\n",
    "    \n",
    "    z = u_x.clone()\n",
    "    z[:, y] = l_x[:, y]\n",
    "    l_spec = cel(z, y)\n",
    "    return kappa*l_fit + (1-kappa)*l_spec\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n",
      "Epoch 0, loss 271.0838898420334\n",
      "Accuracy: 50.0%\n",
      "0.0 1.0\n",
      "Epoch 1, loss 271.0826293826103\n",
      "Accuracy: 50.0%\n",
      "0.0 1.0\n",
      "Epoch 2, loss 271.0798101425171\n",
      "Accuracy: 50.0%\n",
      "0.0 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m kap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m now_e_epoch \u001b[38;5;241m/\u001b[39m gradual_epochs) \u001b[38;5;241m+\u001b[39m kappa \u001b[38;5;241m*\u001b[39m (now_e_epoch \u001b[38;5;241m/\u001b[39m gradual_epochs)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(eps, kap)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     43\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# print(0, x.shape, y.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:42\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_iter)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m GLOVE\u001b[38;5;241m.\u001b[39mstoi:\n\u001b[0;32m---> 20\u001b[0m         emb\u001b[38;5;241m.\u001b[39mappend(\u001b[43mGLOVE\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     22\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# for e in emb:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     print(e.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchtext/vocab/vectors.py:61\u001b[0m, in \u001b[0;36mVectors.__getitem__\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_init \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39mzero_ \u001b[38;5;28;01mif\u001b[39;00m unk_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m unk_init\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache(name, cache, url\u001b[38;5;241m=\u001b[39murl, max_vectors\u001b[38;5;241m=\u001b[39mmax_vectors)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, token):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstoi:\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstoi[token]]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 10\n",
    "kappa = 0.9\n",
    "e_train = 0.01\n",
    "warmup = 3\n",
    "max_e_epoch = 8\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "def test(model, eps):\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            yhat, _, _ = model(x, x-eps, x+eps)\n",
    "            # print('yhat, y', yhat, y)\n",
    "            _, yhat_label = torch.max(yhat, 1)\n",
    "            \n",
    "            num_total += y.shape[0]\n",
    "            num_correct += (y == yhat_label).sum().item()\n",
    "    print(f\"Accuracy: {num_correct / num_total * 100}%\")\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    \n",
    "    now_e_epoch = (\n",
    "        0 if epoch <= warmup\n",
    "        else epoch - warmup if warmup < epoch <= max_e_epoch\n",
    "        else max_e_epoch - warmup\n",
    "    )\n",
    "    gradual_epochs = max_e_epoch - warmup\n",
    "    eps = e_train * (now_e_epoch / gradual_epochs)\n",
    "    kap = 1 * (1 - now_e_epoch / gradual_epochs) + kappa * (now_e_epoch / gradual_epochs)\n",
    "    print(eps, kap)\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # print(0, x.shape, y.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        yhat, l_yhat, u_yhat = model(x, x-eps, x+eps)\n",
    "        loss = criterion(yhat, l_yhat, u_yhat, y, kap)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        sum_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, loss {sum_loss}\")\n",
    "    test(model, eps)\n",
    "torch.save(model, \"model3_2.pth\")\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
