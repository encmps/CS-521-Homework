{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/root/miniconda3/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/root/miniconda3/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/root/miniconda3/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/root/miniconda3/lib/python3.10/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchtext import datasets\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 64\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# reference: https://zhuanlan.zhihu.com/p/562565880\n",
    "train_dataset, test_dataset = datasets.IMDB(root='./IMDB_data', split=('train', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokenizer('UIUC Siebel School.')\n",
    "\n",
    "GLOVE_DIM = 100\n",
    "GLOVE = GloVe(name='6B', dim=GLOVE_DIM)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    labels, texts = list(), list()\n",
    "    for label, text in batch:\n",
    "        # print('label', label)\n",
    "        label = label - 1\n",
    "        assert label in [0, 1]\n",
    "        \n",
    "        tokens = tokenizer(text)\n",
    "        emb = list()\n",
    "        for token in tokens:\n",
    "            if token in GLOVE.stoi:\n",
    "                emb.append(GLOVE[token])\n",
    "        \n",
    "        labels.append(label)\n",
    "        # for e in emb:\n",
    "        #     print(e.shape)\n",
    "        texts.append(torch.stack(emb))\n",
    "        \n",
    "    labels = torch.tensor(labels)\n",
    "    # print(texts)\n",
    "    # print(f'len(texts)={len(texts)}')\n",
    "    texts = pad_sequence(texts, batch_first=True)\n",
    "    return texts, labels\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: IBP Modified Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(100, 100)\n",
    "        self.fc1 = nn.Linear(100, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.output = nn.Linear(100, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def _prop_affine(self, l_x, u_x, W, b):\n",
    "        sum = (u_x + l_x) / 2\n",
    "        dif = (u_x - l_x) / 2\n",
    "        \n",
    "        pos = torch.matmul(sum, W.T) + b.unsqueeze(0)\n",
    "        neg = torch.matmul(dif, torch.abs(W).T)\n",
    "        \n",
    "        l_x = pos - neg\n",
    "        u_x = pos + neg\n",
    "        return l_x, u_x\n",
    "    \n",
    "    def _prop_average(self, l_x, u_x, dim=2):\n",
    "        lower = torch.mean(l_x, dim=dim)\n",
    "        upper = torch.mean(u_x, dim=dim)\n",
    "        return lower, upper\n",
    "    \n",
    "    def _prop_relu(self, l_x, u_x):\n",
    "        l_x = torch.max(l_x, torch.zeros_like(l_x))\n",
    "        u_x = torch.max(u_x, torch.zeros_like(u_x))\n",
    "        return l_x, u_x\n",
    "    \n",
    "    def _prop_softmax(self, l_x, u_x, dim=1):\n",
    "        upper_1 = torch.exp(u_x)\n",
    "        upper_2 = torch.sum(torch.exp(l_x), dim=dim, keepdim=True) - torch.exp(l_x) + torch.exp(u_x)\n",
    "        upper = upper_1 / upper_2\n",
    "        \n",
    "        lower_1 = torch.exp(l_x)\n",
    "        lower_2 = torch.sum(torch.exp(u_x), dim=dim, keepdim=True) - torch.exp(u_x) + torch.exp(l_x)\n",
    "        lower = lower_1 / lower_2\n",
    "        return lower, upper\n",
    "    \n",
    "    def forward(self, x, l_x, u_x):\n",
    "        # print(1, l_x.shape)\n",
    "        l_x, u_x = self._prop_affine(l_x, u_x, self.fc0.weight, self.fc0.bias)\n",
    "        # print(2, l_x.shape)\n",
    "        l_x, u_x = self._prop_relu(l_x, u_x)\n",
    "        l_x, u_x = self._prop_average(l_x, u_x, dim=1)\n",
    "        # print(3, l_x.shape)\n",
    "        \n",
    "        l_x, u_x = self._prop_affine(l_x, u_x, self.fc1.weight, self.fc1.bias)\n",
    "        l_x, u_x = self._prop_relu(l_x, u_x)\n",
    "        l_x, u_x = self._prop_affine(l_x, u_x, self.fc2.weight, self.fc2.bias)\n",
    "        l_x, u_x = self._prop_relu(l_x, u_x)\n",
    "        l_y, u_y = self._prop_affine(l_x, u_x, self.output.weight, self.output.bias)\n",
    "        l_y, u_y = self._prop_softmax(l_y, u_y, dim=1)\n",
    "        \n",
    "        x = self.fc0(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x1 = F.relu(self.fc1(x))\n",
    "        x2 = F.relu(self.fc2(x1))\n",
    "        y = self.softmax(self.output(x2))\n",
    "        \n",
    "        return y, l_y, u_y\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "def criterion(x, l_x, u_x, y, kappa):\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    l_fit = cel(x, y)\n",
    "    \n",
    "    z = u_x.clone()\n",
    "    z[:, y] = l_x[:, y]\n",
    "    l_spec = cel(z, y)\n",
    "    return kappa*l_fit + (1-kappa)*l_spec\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 245.58007702231407\n",
      "Accuracy: 50.0%\n",
      "0.0 1.0\n",
      "Epoch 1, loss 254.36994701623917\n",
      "Accuracy: 50.0%\n",
      "0.0 1.0\n",
      "Epoch 2, loss 255.591561794281\n",
      "Accuracy: 50.0%\n",
      "0.0 1.0\n",
      "Epoch 3, loss 255.63893893361092\n",
      "Accuracy: 50.0%\n",
      "0.002 0.9800000000000001\n",
      "Epoch 4, loss 248.1787613928318\n",
      "Accuracy: 50.0%\n",
      "0.004 0.96\n",
      "Epoch 5, loss 247.3724873661995\n",
      "Accuracy: 50.0%\n",
      "0.006 0.9400000000000001\n",
      "Epoch 6, loss 243.39388191699982\n",
      "Accuracy: 50.0%\n",
      "0.008 0.92\n",
      "Epoch 7, loss 243.62676694989204\n",
      "Accuracy: 50.0%\n",
      "0.01 0.9\n",
      "Epoch 8, loss 241.41047129034996\n",
      "Accuracy: 50.0%\n",
      "0.01 0.9\n",
      "Epoch 9, loss 242.0322293639183\n",
      "Accuracy: 50.0%\n",
      "432.3467676639557\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 10\n",
    "kappa = 0.9\n",
    "e_train = 0.01\n",
    "warmup = 3\n",
    "max_e_epoch = 8\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "def test(model, eps):\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            yhat, _, _ = model(x, x-eps, x+eps)\n",
    "            # print('yhat, y', yhat, y)\n",
    "            _, yhat_label = torch.max(yhat, 1)\n",
    "            \n",
    "            num_total += y.shape[0]\n",
    "            num_correct += (y == yhat_label).sum().item()\n",
    "    print(f\"Accuracy: {num_correct / num_total * 100}%\")\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    \n",
    "    now_e_epoch = (\n",
    "        0 if epoch <= warmup\n",
    "        else epoch - warmup if warmup < epoch <= max_e_epoch\n",
    "        else max_e_epoch - warmup\n",
    "    )\n",
    "    gradual_epochs = max_e_epoch - warmup\n",
    "    eps = e_train * (now_e_epoch / gradual_epochs)\n",
    "    kap = 1 * (1 - now_e_epoch / gradual_epochs) + kappa * (now_e_epoch / gradual_epochs)\n",
    "    print(eps, kap)\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # print(0, x.shape, y.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        yhat, l_yhat, u_yhat = model(x, x-eps, x+eps)\n",
    "        loss = criterion(yhat, l_yhat, u_yhat, y, kap)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        sum_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, loss {sum_loss}\")\n",
    "    test(model, eps)\n",
    "torch.save(model, \"model3_2.pth\")\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('model3_2.pth')\n",
    "\n",
    "def interval_eval(model, eps):\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            _, l_y, u_y = model(x, x-eps, x+eps)\n",
    "            num_total += y.shape[0]\n",
    "            u_y[torch.arange(u_y.shape[0]), y] = l_y[torch.arange(l_y.shape[0]), y] - 100\n",
    "            # print(y.shape, l_y.shape, l_y[:, y].shape, torch.max(u_y, dim=1)[0].shape, (l_y[:, y] > torch.max(u_y, dim=1)[0]).sum().item())\n",
    "            num_correct += (l_y[torch.arange(l_y.shape[0]), y] > torch.max(u_y, dim=1)[0]).sum().item()\n",
    "    print(f\"Accuracy: {num_correct / num_total * 100}%\")\n",
    "interval_eval(model, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
